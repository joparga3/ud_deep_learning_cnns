<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Jose Parreno Garcia" />


<title>Intro to CNN</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">CNNs</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">CNNs</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Intro to CNN</h1>
<h4 class="author"><em>Jose Parreno Garcia</em></h4>
<h4 class="date"><em>March 2018</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#intro-to-convolutional-neural-networks-cnn"><span class="toc-section-number">1</span> Intro to Convolutional Neural Networks (CNN)</a><ul>
<li><a href="#key-features"><span class="toc-section-number">1.1</span> Key features</a></li>
<li><a href="#architecture-overview"><span class="toc-section-number">1.2</span> Architecture overview</a></li>
<li><a href="#convolutional-layers"><span class="toc-section-number">1.3</span> Convolutional layers</a></li>
<li><a href="#pooling-layers"><span class="toc-section-number">1.4</span> Pooling layers</a></li>
<li><a href="#fully-connected-layers"><span class="toc-section-number">1.5</span> Fully-connected layers</a></li>
<li><a href="#architecture-design"><span class="toc-section-number">1.6</span> Architecture design</a></li>
<li><a href="#common-cnn-techniques"><span class="toc-section-number">1.7</span> Common CNN techniques</a></li>
<li><a href="#understanding-and-visualizing-cnns"><span class="toc-section-number">1.8</span> Understanding and visualizing CNNs</a></li>
</ul></li>
<li><a href="#cnns-in-r"><span class="toc-section-number">2</span> CNNs in R</a></li>
<li><a href="#extra"><span class="toc-section-number">3</span> Extra</a></li>
</ul>
</div>

<style>
body {
text-align: justify}
</style>
<p><br></p>
<pre class="r"><code>library(knitr)</code></pre>
<p>In this blog we are going to talk about</p>
<ul>
<li>Intro to Convolutional Neural Networks (CNN)</li>
<li>CNNs in R</li>
<li>Example case: classifying images</li>
</ul>
<p><br></p>
<div id="intro-to-convolutional-neural-networks-cnn" class="section level1">
<h1><span class="header-section-number">1</span> Intro to Convolutional Neural Networks (CNN)</h1>
<div id="key-features" class="section level2">
<h2><span class="header-section-number">1.1</span> Key features</h2>
<p><img src="images/1.PNG" width="722" /></p>
<p><br></p>
<p><img src="images/2.PNG" width="722" /></p>
</div>
<div id="architecture-overview" class="section level2">
<h2><span class="header-section-number">1.2</span> Architecture overview</h2>
<p><img src="images/3.PNG" width="709" /></p>
<p><br></p>
<p><img src="images/4.PNG" width="712" /></p>
</div>
<div id="convolutional-layers" class="section level2">
<h2><span class="header-section-number">1.3</span> Convolutional layers</h2>
<p><img src="images/5.PNG" width="716" /></p>
<p><br></p>
<p>Let try to explain that in numerical terms:</p>
<ul>
<li>Lets say the kernel matrix is the filter we want to use (same like extracting the edges in the previous image)</li>
<li>Then we have the real input data</li>
<li>We can pick all combinations of matrices 3x3 in the input data and filter it using the matrix kernel by doing an matrix-element multiplication</li>
<li>The value 4 is the value of the pixel in the convoluted image</li>
<li>If you remember, this is the same operation as the perceptron does, so in order to obtain the convolved image, we need a perceptron for each output pixel BUT ALL OF THESE PERCEPTRONS SHARE THE SAME WEIGHTS! Which is what we saw in the key features.</li>
</ul>
<p><img src="images/6.PNG" width="720" /></p>
<p><br></p>
<p><img src="images/7.PNG" width="740" /></p>
<p><br></p>
<p><img src="images/8.PNG" width="713" /></p>
<p><br></p>
<p><img src="images/9.PNG" width="735" /></p>
</div>
<div id="pooling-layers" class="section level2">
<h2><span class="header-section-number">1.4</span> Pooling layers</h2>
<p><img src="images/10.PNG" width="728" /></p>
<p><br></p>
<p><img src="images/11.PNG" width="717" /></p>
</div>
<div id="fully-connected-layers" class="section level2">
<h2><span class="header-section-number">1.5</span> Fully-connected layers</h2>
<p><img src="images/12.PNG" width="729" /></p>
</div>
<div id="architecture-design" class="section level2">
<h2><span class="header-section-number">1.6</span> Architecture design</h2>
<p><img src="images/13.PNG" width="730" /></p>
<p><br></p>
<p><img src="images/14.PNG" width="724" /></p>
<p><br></p>
<p><img src="images/15.PNG" width="712" /></p>
</div>
<div id="common-cnn-techniques" class="section level2">
<h2><span class="header-section-number">1.7</span> Common CNN techniques</h2>
<p><img src="images/16.PNG" width="734" /></p>
<p><br></p>
<p><img src="images/17.PNG" width="743" /></p>
</div>
<div id="understanding-and-visualizing-cnns" class="section level2">
<h2><span class="header-section-number">1.8</span> Understanding and visualizing CNNs</h2>
<p><img src="images/18.PNG" width="736" /></p>
<p>A way to do this when dealing with image recognition is to plot what is being outputed after each layer. You can see at the beggining, the activations seem like dots, clouds or blobs (right image), but then bit by bit, the activation functions start localizing and grouping.</p>
<p>One thing to notice from the right image is for example, the black boxes. These black boxes indicate that the activation function has outputted majority of 0, which may not be true, and just a consequence of having used too much of a big learn rate.</p>
<p><img src="images/19.PNG" width="689" /></p>
<p>Another way to understand CNN is to visualize the weights, instead of the actual output. The weights are useful to visualize because well trained networks usually display nice smooth filters without any noise (noise patterns can be an indicator of a network that isnt being trained enough or using a very low regulirization strength which may lead to overfitting)</p>
<p><img src="images/20.PNG" width="683" /></p>
<p>Another visualization technique is to take a large daat set of images and feed them through the network, keeping track of which images maximally activates a neuron. We can then visualise the images to get an understanding of what a neuron is looking for in its receptive field. In the following screenshot, you can see that the first row the neuron is looking for faces, then dog faces, etc.</p>
<p><img src="images/21.PNG" width="758" /></p>
<p>Suppose that the CNN classifies an image as a dog: how can we be certain that it’s choosing the right class based on the feature of the dog as opposed to some other objects. Imaginge we pick a grey square and add it as a patch to the image of interest. WE can do that and calculate the probability of whatever we want to calculate. For example, in the left image we can see that the probability of a dog when the patch covers the face of dog drops dramatically. In other words, the grey patch at every position shows a red heat map because we are certain we have a dog in the imagine, except when the patch covers the dogs face. This basically tells us that, in order to classify a dog, the most important bit is the dogs face!</p>
<p><img src="images/22.PNG" width="509" /></p>
<p><br></p>
</div>
</div>
<div id="cnns-in-r" class="section level1">
<h1><span class="header-section-number">2</span> CNNs in R</h1>
<p>In this section we will learn how to implement a Lenet7 with the MXNet package on the MNIST dataset in order to improve the accuracy on the recognition of handwritten digits.</p>
<p>Let’s import part of the the MNIST dataset:</p>
<pre class="r"><code># install.packages(&quot;drat&quot;)
# drat::addRepo(&quot;dmlc&quot;)
# 
# install.packages(&quot;https://github.com/jeremiedb/mxnet_winbin/raw/master/mxnet.zip&quot;, repos = NULL)library(mxnet)
require(mxnet)</code></pre>
<pre><code>## Loading required package: mxnet</code></pre>
<pre><code>## Warning: package &#39;mxnet&#39; was built under R version 3.4.3</code></pre>
<pre class="r"><code>require(caret)</code></pre>
<pre><code>## Loading required package: caret</code></pre>
<pre><code>## Warning: package &#39;caret&#39; was built under R version 3.4.3</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.4.1</code></pre>
<p><img src="images/23.PNG" width="710" /></p>
<p>And split the dataset in train and test set (you can also create the validation set to be more polite at home)</p>
<pre class="r"><code>dataset &lt;- read.csv(&#39;train.csv&#39;, header=TRUE)
dim(dataset)</code></pre>
<pre><code>## [1] 42000   785</code></pre>
<pre class="r"><code>train &lt;- data.matrix(dataset[1:5000,])
test &lt;- data.matrix(dataset[5000:10000,])

train.x &lt;- train[,-1]
train.y &lt;- train[,1]

test.x &lt;- test[,-1]
test.y &lt;- test[,1]

train.x &lt;- t(train.x/255)
test.x &lt;- t(test.x/255)</code></pre>
<p>Than we can create define the Lenet7:</p>
<pre class="r"><code># input
data &lt;- mx.symbol.Variable(&#39;data&#39;)

# first conv
conv1 &lt;- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20)
tanh1 &lt;- mx.symbol.Activation(data=conv1, act_type=&quot;tanh&quot;)
pool1 &lt;- mx.symbol.Pooling(data=tanh1, pool_type=&quot;max&quot;,
                          kernel=c(2,2), stride=c(2,2))
# second conv
conv2 &lt;- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 &lt;- mx.symbol.Activation(data=conv2, act_type=&quot;tanh&quot;)
pool2 &lt;- mx.symbol.Pooling(data=tanh2, pool_type=&quot;max&quot;,
                          kernel=c(2,2), stride=c(2,2))
# first fullc
flatten &lt;- mx.symbol.Flatten(data=pool2)
fc1 &lt;- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 &lt;- mx.symbol.Activation(data=fc1, act_type=&quot;tanh&quot;)
# second fullc
fc2 &lt;- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
# loss
lenet &lt;- mx.symbol.SoftmaxOutput(data=fc2)

#Then let us reshape the matrices into arrays:
train.array &lt;- train.x
dim(train.array) &lt;- c(28, 28, 1, ncol(train.x))
test.array &lt;- test.x
dim(test.array) &lt;- c(28, 28, 1, ncol(test.x))</code></pre>
<p>And than training the CNN:</p>
<pre class="r"><code>mx.set.seed(0)
tic &lt;- proc.time()
model &lt;- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
                                     num.round=10, array.batch.size=100,
                                     ctx=mx.cpu(), learning.rate=0.05, momentum=0.9,
                                     wd=0.00001, eval.metric=mx.metric.accuracy,
         epoch.end.callback=mx.callback.log.train.metric(100))</code></pre>
<pre><code>## Start training with 1 devices</code></pre>
<pre><code>## [1] Train-accuracy=0.104285714285714</code></pre>
<pre><code>## [2] Train-accuracy=0.1042</code></pre>
<pre><code>## [3] Train-accuracy=0.1042</code></pre>
<pre><code>## [4] Train-accuracy=0.1294</code></pre>
<pre><code>## [5] Train-accuracy=0.6298</code></pre>
<pre><code>## [6] Train-accuracy=0.8852</code></pre>
<pre><code>## [7] Train-accuracy=0.9344</code></pre>
<pre><code>## [8] Train-accuracy=0.9584</code></pre>
<pre><code>## [9] Train-accuracy=0.968</code></pre>
<pre><code>## [10] Train-accuracy=0.9802</code></pre>
<pre class="r"><code>preds &lt;- predict(model, test.array)
dim(preds)</code></pre>
<pre><code>## [1]   10 5001</code></pre>
<pre class="r"><code>pred &lt;- apply(preds, 2, which.max)
pred &lt;- pred-1
caret::confusionMatrix(pred, test.y)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1   2   3   4   5   6   7   8   9
##          0 485   0   0   0   0   2   6   1   1   0
##          1   0 526   2   1   0   0   0   2   2   1
##          2   1   4 484   7   4   0   0  13   6   0
##          3   0   0   1 502   0   6   0   1   3   1
##          4   0   1   1   0 464   0   1   3   0   3
##          5   2   0   1   9   0 422   3   0   2   3
##          6   5   2   2   0   6   5 475   0   4   0
##          7   0   1   3   4   0   0   0 504   3  11
##          8   3   2   6   6   2   2   2   1 448   2
##          9   1   1   0   0  14   0   0   8   4 497
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9612          
##                  95% CI : (0.9555, 0.9664)
##     No Information Rate : 0.1074          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9569          
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9
## Sensitivity           0.97586   0.9795  0.96800   0.9490  0.94694  0.96568  0.97536   0.9456  0.94715  0.95946
## Specificity           0.99778   0.9982  0.99222   0.9973  0.99800  0.99562  0.99468   0.9951  0.99426  0.99375
## Pos Pred Value        0.97980   0.9850  0.93256   0.9767  0.98097  0.95475  0.95190   0.9582  0.94515  0.94667
## Neg Pred Value        0.99734   0.9975  0.99643   0.9940  0.99426  0.99671  0.99733   0.9935  0.99448  0.99531
## Prevalence            0.09938   0.1074  0.09998   0.1058  0.09798  0.08738  0.09738   0.1066  0.09458  0.10358
## Detection Rate        0.09698   0.1052  0.09678   0.1004  0.09278  0.08438  0.09498   0.1008  0.08958  0.09938
## Detection Prevalence  0.09898   0.1068  0.10378   0.1028  0.09458  0.08838  0.09978   0.1052  0.09478  0.10498
## Balanced Accuracy     0.98682   0.9889  0.98011   0.9731  0.97247  0.98065  0.98502   0.9703  0.97070  0.97661</code></pre>
<p><br></p>
</div>
<div id="extra" class="section level1">
<h1><span class="header-section-number">3</span> Extra</h1>
<p><img src="images/24.PNG" width="726" /></p>
<p><br></p>
<p><img src="images/25.PNG" width="729" /></p>
<p><br></p>
<p><img src="images/26.PNG" width="723" /></p>
<p><br></p>
<p><img src="images/27.PNG" width="722" /></p>
<p><br></p>
<p><img src="images/28.PNG" width="767" /></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
